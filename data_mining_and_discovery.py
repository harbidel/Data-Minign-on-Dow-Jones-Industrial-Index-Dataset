# -*- coding: utf-8 -*-
"""Data Mining and Discovery.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12tpXwQn-2afAl_tmvxisddXS1YvQYXZg
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.cluster import KMeans

data = pd.read_csv('/content/dow_jones_index.data')
data.head()

data.info()

# Remove the dollar sign from relevant columns
columns_to_remove_dollar_sign = ['open',	'high', 'low', 'close', 'volume', 'percent_change_price', 'percent_change_volume_over_last_wk',
                                 'previous_weeks_volume', 'next_weeks_open', 'next_weeks_close', 'percent_change_next_weeks_price',
                                 'days_to_next_dividend', 'percent_return_next_dividend']

for col in columns_to_remove_dollar_sign:
    data[col] = data[col].astype(str).str.replace('$', '')

# Convert columns to the appropriate data types if necessary
data['volume'] = data['volume'].astype(int)
data['previous_weeks_volume'] = data['previous_weeks_volume'].astype(float)
data['next_weeks_open'] = data['next_weeks_open'].astype(float)
data['next_weeks_close'] = data['next_weeks_close'].astype(float)

# Print the updated dataset
data.head()

data.info()

# Drop the "Stock" and "Date" columns
data = data.drop(['stock', 'date'], axis=1)

# Print the updated dataset
data.head()

data.info()

# Convert object columns to numerical data type
data['open'] = pd.to_numeric(data['open'], errors='coerce')
data['high'] = pd.to_numeric(data['high'], errors='coerce')
data['low'] = pd.to_numeric(data['low'], errors='coerce')
data['close'] = pd.to_numeric(data['close'], errors='coerce')
data['volume'] = pd.to_numeric(data['volume'], errors='coerce')
data['percent_change_price'] = pd.to_numeric(data['percent_change_price'], errors='coerce')
data['percent_change_volume_over_last_wk'] = pd.to_numeric(data['percent_change_volume_over_last_wk'], errors='coerce')
data['previous_weeks_volume'] = pd.to_numeric(data['previous_weeks_volume'], errors='coerce')
data['next_weeks_open'] = pd.to_numeric(data['next_weeks_open'], errors='coerce')
data['next_weeks_close'] = pd.to_numeric(data['next_weeks_close'], errors='coerce')
data['percent_change_next_weeks_price'] = pd.to_numeric(data['percent_change_next_weeks_price'], errors='coerce')
data['days_to_next_dividend'] = pd.to_numeric(data['days_to_next_dividend'], errors='coerce')
data['percent_return_next_dividend'] = pd.to_numeric(data['percent_return_next_dividend'], errors='coerce')

# Print the updated dataset
data.head()

data.info()

# Impute missing values with the mean
imputer = SimpleImputer(strategy='mean')
data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)

# Scale the features
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

"""## Apply K-means clustering:"""

# Instantiate K-means with the desired number of clusters
kmeans = KMeans(n_clusters=5)

# Fit the data to the K-means model
kmeans.fit(scaled_data)

# Get the cluster labels
kmeans_labels = kmeans.labels_

# Compute the silhouette score for K-means
kmeans_silhouette = silhouette_score(scaled_data, kmeans_labels)
print("K-means Silhouette Score:", kmeans_silhouette)

"""## Apply DBSCAN:"""

# Instantiate DBSCAN with adjusted parameters
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Fit the data to the DBSCAN model
dbscan.fit(scaled_data)

# Get the cluster labels (-1 represents noise/outliers)
dbscan_labels = dbscan.labels_

# Compute the number of clusters (excluding noise/outliers)
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
print("Number of clusters:", n_clusters)

# Compute the silhouette score for DBSCAN if there are at least two unique labels
if n_clusters > 1:
    dbscan_silhouette = silhouette_score(scaled_data, dbscan_labels)
    print("DBSCAN Silhouette Score:", dbscan_silhouette)
else:
    print("Insufficient clusters to compute silhouette score.")

"""## Visualize the results:"""

# Plot the K-means clusters
plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=kmeans_labels)
plt.title("K-means Clustering")
plt.show()

# Plot the DBSCAN clusters
plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=dbscan_labels)
plt.title("DBSCAN Clustering")
plt.show()

# Plot the K-means clusters based on all columns
num_columns = scaled_data.shape[1]
fig, axs = plt.subplots(num_columns, num_columns, figsize=(15, 15))

for i in range(num_columns):
    for j in range(num_columns):
        axs[i, j].scatter(scaled_data[:, i], scaled_data[:, j], c=kmeans_labels)
        axs[i, j].set_xlabel(data.columns[i])
        axs[i, j].set_ylabel(data.columns[j])

plt.suptitle("K-means Clustering based on all Columns")
plt.tight_layout()
plt.show()

# Plot the DBSCAN clusters based on all columns
fig, axs = plt.subplots(num_columns, num_columns, figsize=(15, 15))

for i in range(num_columns):
    for j in range(num_columns):
        axs[i, j].scatter(scaled_data[:, i], scaled_data[:, j], c=dbscan_labels)
        axs[i, j].set_xlabel(data.columns[i])
        axs[i, j].set_ylabel(data.columns[j])

plt.suptitle("DBSCAN Clustering based on all Columns")
plt.tight_layout()
plt.show()

